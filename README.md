# ğŸ§  Information Retrieval System using Local LLM (Ollama + Mistral) and HuggingFace Embeddings

A privacy-preserving, conversational AI system that allows users to upload multiple PDF documents and ask context-aware questions based on their content. It leverages **local LLMs (via Ollama)** and **HuggingFace sentence embeddings**, making it ideal for offline or sensitive environments.

---

## ğŸš€ Features

- ğŸ“ Upload & process multiple PDF documents
- âœ‚ï¸ Chunk and embed documents using "all-MiniLM-L6-v2" from HuggingFace
- ğŸ” Store chunks in a FAISS vector store
- ğŸ¤– Interact with documents using **Mistral LLM via Ollama**
- ğŸ§  Maintains conversation history for multi-turn QA
- âš™ï¸ Fully local & offline-capable â€” no external API calls
- ğŸ–¥ï¸ Simple UI built using Streamlit

---

## ğŸ§± Tech Stack

| Component            | Technology                         |
|---------------------|-------------------------------------|
| UI                  | Streamlit                           |
| LLM                 | Mistral (via Ollama)                |
| Embeddings          | all-MiniLM-L6-v2 (HuggingFace)      |
| Vector Database     | FAISS (via LangChain)               |
| PDF Parsing         | PyPDF2                              |
| Memory              | ConversationBufferMemory            |
| Environment Config  | python-dotenv (.env support)        |

---

## ğŸ–¼ï¸ Architecture

```mermaid
graph TD
    A[User Uploads PDFs] --> B[Extract Text with PyPDF2]
    B --> C[Split Text into Chunks]
    C --> D[Embed Chunks using HuggingFace MiniLM]
    D --> E[Store Chunks in FAISS Vector Store]
    F[User Enters Question] --> G[Retrieve Relevant Chunks]
    G --> H[Answer using Ollama LLM (Mistral)]
    H --> I[Stream Answer via Streamlit UI]
```
---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/AftabSahil/Information-Retrival-System.git
cd Information-Retrival-System
```
---
### 2. Create and activate a virtual environment

```bash
python -m venv venv
source venv/bin/activate      # for Linux/macOS
venv\Scripts\activate         # for Windows
```
---
### 3. Install dependencies
```
pip install -r requirements.txt
```
---
### 4. Install Ollama (if not installed)
Visit: https://ollama.com/download
Then run:
```
ollama run mistral
This downloads and serves the Mistral model locally via the Ollama CLI.
```

---
### 5. Run the application
```bash
streamlit run app.py
```
---
### ğŸ’» How It Works
```
Upload multiple PDF documents via the UI

System reads and extracts the full text

Text is chunked for better context resolution

Chunks are converted to vector embeddings and stored in FAISS

You ask questions; system retrieves relevant chunks

Mistral generates answers based on the content context

Responses are displayed in a chat-like format
```
---
### ğŸ§ª Use Case Examples
ğŸ“š Research assistant for reading academic papers

ğŸ§¾ Contract summarizer for legal documents

ğŸ“Š QA assistant for company reports

ğŸ“• Notes reader for students
---
### ğŸ“‚ Project Structure
```
â”œâ”€â”€ app.py                      # Streamlit frontend
â”œâ”€â”€ src/
â”‚   â””â”€â”€ helper.py              # Core logic (PDF, Embeddings, LLM)
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ .env                       # Environment variables (if needed)
â””â”€â”€ README.md                  # You're here
```
